# ai-alignment-project

This github repo heavily relies on code from https://github.com/andyzoujm/representation-engineering and https://github.com/saprmarks/geometry-of-truth

Using Python 3.11

- [x] Set up access to model internals
- [x]  Set up environments
- [x]  Set up Avalon-LLM
  - [x] Get it to run locally
  - [ ] Testing activation patching
  - [ ] Testing Mistral involvement
 - [x]  Get training data (2 truths 1 lie)
  - [x] Prompt experimentation
    - [x] LLaMA (Bruce)
    - [x] Mistral (Bruce)
    - [x] Test with DSPy (Rob)
    - [ ] Test with getting GPT-4 to provide few-shot prompting?
  - [x] Get datasets with response + hidden representations
    - [ ] Make dataset of 2 truths and a lie (each row is 3 sentences, a label for each, and then the group has a label (Stephen)
- [ ] RepE 
  - [x] testing using RepE dataset (simplified) (Rob)
  - [x] Testing on two truths and a lie dataset (Bruce)
  - [ ] Run to get honesty eval on 2 truths and a lie (Stephen)
  - [ ] Visualize (Stephen)
  - [ ] Test on Avalon?
- [ ] Training Approaches
  - [ ] Make dataset of N truths M lies
    - [ ] One and one, two and one, two and two
  - [ ] Train on those (Rob)
  - [ ] See performance
- [ ] Write-up
 - [ ] Abstract
 - [ ] Figure/Diagram
 - [ ] Formal Description
 - [ ] Related Work
 - [ ]   

